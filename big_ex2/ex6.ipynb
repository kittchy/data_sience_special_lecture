{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データサイエンス特論　プログラム課題　第2回\n",
    "\n",
    "情報知能工学専攻　音声言語処理研究室\n",
    "\n",
    "M203319 M1 木内貴浩 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の課題を実施し、実行したプログラムと実行結果(プログラムはPython, Jupyter notebook(拡張子.ipynb, .html)等でログをまとめるのも可、実行結果を MS Word(PDF)等でま とめることも可) を、ZIP でまとめ、MoodleLMS にアップロードすること。期限は【8 月 8 日(月)】の深夜 23:59 分までとする。(プログラム課題はこれで終わりです)\n",
    "\n",
    "\n",
    "UCI(カリフォルニア大学アーバイン校)の機械学習用データレポジトリにある有名なロ イ タ ー の ニ ュ ー ス 記 事 ( Reuters-21578 ) https://archive.ics.uci.edu/ml/datasets/reuters- 21578+text+categorization+collection は、英語で書かれた1980 年当時のニュース記事の 集合 である。このデータは、機械学習の練習にしばしば利用されてきた。ニュース記事には カ テゴリー情報が 1 種類以上付与(マルチラベルが付与)されており、このカテゴリーラベ ルを予測するテキスト分類問題を考える。ただし、元のロイターニュース記事には、各種 のノイズ文書がある。そこで、55 カテゴリー、10,700 文書(7713 訓練文書, 2987 テスト文 書)に絞り込んだ 部分集合記事データを作成した。以下のサイトからダウンロードで き る 。 https://www.kde.cs.tut.ac.jp/~aono/data/ma_reuters.zip においてある。 以降、こ のデータをMA-Reuters と呼ぶこととする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 MA-Reutersを各自の作業できるPC等にダウンロードせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kinouchitakahiro/Documents/Personal\n"
     ]
    }
   ],
   "source": [
    "!cd ../datas && wget https://www.kde.cs.tut.ac.jp/~aono/data/ma_reuters.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 環境準備\n",
    "\n",
    "MA-ReutersはPythonのNLTKパッケージのコーパスに従って作成してある。そこで、各自のPython の環境にNLTK がまだインストールされていない場合、NLTKをインストールせよ。\n",
    "\n",
    "同時に https://www.nltk.org/data.html を参照してデータのダウンロードを最初に一回行い、適当な NLTK 用のデータフォルダーを作成しておくこと。た とえば Windows 環境下で、C:\\nltk_data に作成したとすると、コーパス フォルダが、 C:\\nltk_data\\corpora 以下にできるので、(1)で入手したma_reuters.zip を corpora フォルダ にコピーしておくこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Scikit-learnのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ma_reuters の移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv ../datas/ma_reuters.zip /Users/kinouchitakahiro/nltk_data/corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題1 \n",
    "\n",
    "https://www.kde.cs.tut.ac.jp/~aono/2022/DataScience/Reuters-Multi-Label-SVM-Example.html に ある Jupyter notebook でのPython コードを順番に実行し、多値クラス多値ラベル問題を 解決できることを確認せよ。ただし、このコード内でオレンジカテゴリー内の(最初の)文書 のプリントは、違うカテゴリー(オレンジカテゴリー以外)の文書に変更して実行するこ と。\n",
    "\n",
    "## NLTKを使ったコーパスのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700  記事総数\n",
      "7713 訓練データ\n",
      "2987 テストデータ\n",
      "55 カテゴリー\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'cocoa', 'coffee', 'copper', 'corn', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'hog', 'housing', 'interest', 'ipi', 'iron-steel', 'jobs', 'lead', 'livestock', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'oilseed', 'orange', 'palm-oil', 'pet-chem', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "\n",
    "# コーパスのロード\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# MA_Reutersのロード\n",
    "documents = ma_reuters.fileids()\n",
    "print( f\"{str(len(documents))}  記事総数\")\n",
    "\n",
    "# 訓練とテストデータの文書IDの抽出\n",
    "train_docs_id = [doc for doc in documents if doc.startswith(\"train\")]\n",
    "test_docs_id = [doc for doc in documents if doc.startswith(\"test\")]\n",
    "\n",
    "print( f\"{str(len(train_docs_id))} 訓練データ\")\n",
    "print( f\"{str(len(test_docs_id))} テストデータ\")\n",
    "\n",
    "# 訓練とテストデータの生データの抽出\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    " \n",
    "# カテゴリーのリスト\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "\n",
    "print( f\"{num_categories} カテゴリー\")\n",
    "print( f\"{categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shipのカテゴリーにある記事を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUSTRALIAN FOREIGN SHIP BAN ENDS BUT NSW PORTS HIT\n",
      "  Tug crews in New South Wales (NSW),\n",
      "  Victoria and Western Australia yesterday lifted their ban on\n",
      "  foreign-flag ships carrying containers but NSW ports are still\n",
      "  being disrupted by a separate dispute, shipping sources said.\n",
      "      The ban, imposed a week ago over a pay claim, had prevented\n",
      "  the movement in or out of port of nearly 20 vessels, they said.\n",
      "      The pay dispute went before a hearing of the Arbitration\n",
      "  Commission today.\n",
      "      Meanwhile, disruption began today to cargo handling in the\n",
      "  ports of Sydney, Newcastle and Port Kembla, they said.\n",
      "      The industrial action at the NSW ports is part of the week\n",
      "  of action called by the NSW Trades and Labour Council to\n",
      "  protest changes to the state's workers' compensation laws.\n",
      "      The shipping sources said the various port unions appear to\n",
      "  be taking it in turn to work for a short time at the start of\n",
      "  each shift and then to walk off.\n",
      "      Cargo handling in the ports has been disrupted, with\n",
      "  container movements most affected, but has not stopped\n",
      "  altogether, they said.\n",
      "      They said they could not say how long the disruption will\n",
      "  go on and what effect it will have on shipping movements.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生の文書例（オレンジ・カテゴリー）\n",
    "# Documents in a category\n",
    "category_docs = ma_reuters.fileids(\"ship\");\n",
    "document_id = category_docs[0] # ship・カテゴリーの最初の文書\n",
    "\n",
    "# 記事の中身を表示\n",
    "print( f\"{ma_reuters.raw(document_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTKを用いたテキストのトークン抽出, scikit-learnを用いたテキストのTF-IDFモデル抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IFモデルに変換しました\n",
      "訓練データの文書数x次元数：(7713, 26978)\n",
      "テストデータの文書数x次元数：(2987, 26978)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import re # 正規表現\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "def tokenize(text): # テキストを小文字英語トークンに変換してリストで返す関数\n",
    "    min_length = 3 # 3文字以上のものだけ残す\n",
    "    words = [word.lower() for word in word_tokenize(text)]\n",
    "    p = re.compile('[a-zA-Z]+') # 小文字化しているが一応アルファベットで開始\n",
    "    filtered_tokens = [token for token in words \\\n",
    "                       if p.match(token) and len(token) >= min_length]\n",
    "    return filtered_tokens\n",
    "\n",
    "# TF-IDF重みでベクトル化\n",
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "\n",
    "# 訓練データはfit_transform関数で決められた語彙に基づきTF-IDFを計算\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "\n",
    "# テストデータはtransform関数で決められた語彙に基づきTF-IDFを計算\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "print(f\"TF-IFモデルに変換しました\")\n",
    "print(f\"訓練データの文書数x次元数：{vectorised_train_documents.shape}\")\n",
    "print(f\"テストデータの文書数x次元数：{vectorised_test_documents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMによる分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7713, 26978) (7713, 55)\n",
      "Jaccard係数による評価:0.86\n",
      "カテゴリごとのjaccard_scoreの算出\n",
      "max: earn : 0.9690627843494085\n",
      "min: lead : 0.0\n",
      "Hamming損失による評価:0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) \\\n",
    "                             for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) \\\n",
    "                             for doc_id in test_docs_id])\n",
    "print(vectorised_train_documents.shape, train_labels.shape)\n",
    "\n",
    "# マルチクラス　マルチラベル分類器で訓練＋予測\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "# Jaccard係数の計算\n",
    "print (f\"Jaccard係数による評価:\"\n",
    "    f\"{np.round(jaccard_score(test_labels,OVR_predictions, average='samples'),3)}\")\n",
    "\n",
    "print(\"カテゴリごとのjaccard_scoreの算出\")\n",
    "max = [-1  ,\"\"]\n",
    "min = [1000,\"\"]\n",
    "for category, jscore in zip(categories, jaccard_score(test_labels,OVR_predictions, average=None)):\n",
    "    if max[0]<jscore:\n",
    "        max = [jscore, category]\n",
    "    if min[0]> jscore:\n",
    "        min = [jscore, category]\n",
    "\n",
    "print(f\"max: {max[1]} : {max[0]}\")\n",
    "print(f\"min: {min[1]} : {min[0]}\")\n",
    "\n",
    "# Hamming損失の計算\n",
    "print (f\"Hamming損失による評価:\"\n",
    "    f\"{np.round(hamming_loss(test_labels,OVR_predictions),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2\n",
    "\n",
    "55個のカテゴリーのなかで、Jaccard 係数(資料第 6 回参照)が最も高いカテゴリーと最 も低いカテゴリーが何であったかのべよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記で算出されたように、Jaccardが最大と最小となったカテゴリは以下の通りとなった。\n",
    "\n",
    "- max: earn : 0.9690627843494085\n",
    "- min: lead : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題3\n",
    "\n",
    "ここでのNLTKのTF-IDFモデル+SVM以外の組み合わせで本マルチラベル問題に対応できる手法を適宜ためし、実行結果をのべよ。たとえば、テキスト表現に授業の資料やビデオで述べたWord2VecやBERT等を用いてもよい。NLTKのBoW(TF-IDF)モデルのまま場合、少なくとも分類器は変更すること。たとえば、ニューラルネット(NN)を用いてもよい。NN の場合、評価手法はバイナリークロスエントロピー等の誤差がマルチラベル・マルチクラス分類ではよく用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は今までにずっと使ってみたかったWord2vecを使って分類してみる。\n",
    "\n",
    "使うモジュールはGensimを使用しており、以下のサイトを参考にしている。\n",
    "\n",
    "[gensim](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "また、word2vecの応用版、doc2vecを利用\n",
    "\n",
    "[doc2vec](https://qiita.com/naotaka1128/items/2c4551abfd40e43b0146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "def split_docs(docs: list):\n",
    "    return [list(filter(None, re.split('[ \\n]',td))) for td in docs]\n",
    "\n",
    "\n",
    "# 参考記事： http://qiita.com/okappy/items/32a7ba7eddf8203c9fa1\n",
    "class LabeledListSentence(object):\n",
    "    def __init__(self, words_list, labels):\n",
    "        self.words_list = words_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, words in enumerate(self.words_list):\n",
    "            yield models.doc2vec.TaggedDocument(words, [self.labels[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7713 7713\n"
     ]
    }
   ],
   "source": [
    "splited_train_docs = split_docs(train_docs)\n",
    "splited_test_docs = split_docs(test_docs)\n",
    "\n",
    "train_labels_text = [ma_reuters.categories(doc_id) for doc_id in train_docs_id]\n",
    "print(len(train_labels_text), len(splited_train_docs))\n",
    "\n",
    "train_sentences = [models.doc2vec.TaggedDocument(words, label) for words,label in zip(splited_train_docs, train_labels_text)]\n",
    "\n",
    "\n",
    "model = models.Doc2Vec(documents=train_sentences,vector_size=1000, window=2, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "model.save('doc2vec.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリごとに素性ベクトルを計算\n",
    "\n",
    "カテゴリ別に文章を素性ベクトルに変換（単語ごとに特徴量を加算）する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/kinouchitakahiro/anaconda3/envs/test_sagemaker/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard係数による評価:0.691\n"
     ]
    }
   ],
   "source": [
    "# 学習後はモデルをファイルからロード可能\n",
    "model = models.Doc2Vec.load('doc2vec.model')\n",
    "\n",
    "train_vec_list = []\n",
    "for docs in splited_train_docs:\n",
    "    train_vec_list.append(model.infer_vector(docs))\n",
    "train_vec = np.array(train_vec_list)\n",
    "\n",
    "test_vec_list = []\n",
    "for docs in splited_test_docs:\n",
    "    test_vec_list.append(model.infer_vector(docs))\n",
    "test_vec = np.array(test_vec_list)\n",
    "\n",
    "\n",
    "\n",
    "# # マルチクラス　マルチラベル分類器で訓練＋予測\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41)) \n",
    "OVR_classifier.fit(train_vec, train_labels)\n",
    "\n",
    "OVR_predictions = OVR_classifier.predict(test_vec)\n",
    "\n",
    "# Jaccard係数の計算\n",
    "print (f\"Jaccard係数による評価:\"\n",
    "    f\"{np.round(jaccard_score(test_labels,OVR_predictions, average='samples'),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果と考察\n",
    "\n",
    "Doc2vecを用いて他クラスラベリングのタスクを行った。\n",
    "はじめはDoc2vecの次元数を100程度で行ったが、SVMが収束しなかった.\n",
    "Jaccard係数による評価:0.689　となり、不十分なベクトル表現であると考えられる。\n",
    "\n",
    "\n",
    "そこで、word2vecの次元数を2000まで増やすことで、他クラス分類の精度を向上させることができた。\n",
    "Jaccard係数による評価:0.691\n",
    "\n",
    "計算時間は非常にかかったがJaccard係数の結果はTF-IDFよりも低い結果となってしまった。この原因として、データセットの単語数が十分ではなく、単語の分散表現がうまく学習できていないためと考えられる。\n",
    "今回はSVMを使って分類したが、この分類までNNに任せると、より柔軟に分類されると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test_sagemaker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e8a75cb032066ff259bf00a33521621f5d385b11c85fa830129201b574cccd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
